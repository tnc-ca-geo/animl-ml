{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a TensorFlow SavedModel model trained elsewhere to Amazon SageMaker\n",
    "\n",
    "A lot of the steps below are taken from [this blog post](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/) which explains how to take advantage of Amazon SageMaker deployment capabilities, such as selecting the type and number of instances, performing A/B testing, and Auto Scaling. Auto Scaling clusters are spread across multiple Availability Zones to deliver high performance and high availability.\n",
    "\n",
    "In this notebook we'll be deploying Microsoft's Megadetector model, saved in SavedModel for TF Serving format, which can be downloaded [here](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md#downloading-the-models). The blog post listed above also demonstrates how to deploy Keras models (JSON and weights hdf5) format to Sagemaker, but that is out of the scope of this notebook.\n",
    "\n",
    "For more on training the model on SageMaker and deploying, refer to https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up\n",
    "\n",
    "If you're already reading this in a Sagemaker Notebook instance, just execute the code block below to get the Sagemaker execution role.\n",
    "\n",
    "If not, and you need to set up the Sagemaker Notebook, in the AWS Management Console go to the Amazon SageMaker console. Choose Notebook Instances, and create a new notebook instance. Associate it with the animl-ml git repo (https://github.com/tnc-ca-geo/animl-ml), and set the kernel to conda_tensorflow_p36.\n",
    "\n",
    "The ```get_execution_role``` function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Convert TensorFlow model to a SageMaker readable format\n",
    "\n",
    "Download the megadetector model, unzip it, and make sure the .pb file is named ```saved_model.pb```. Note - you may have already done this if you ran the ```get_models.sh``` script locally.\n",
    "\n",
    "Create an export directory structure in the jupyter environment (```animl-ml/notebooks/export/Servo/1```), and upload the contents of the downloaded model there. Create a code directory in the export folder (```animl-ml/notebooks/export/code```), and **copy** the contents of ```animl-ml/code```  (```inference.py``` and ```requirements.txt``` files) into it. ```inference.py``` is a pre/post processing script, and dependencies in ```requirements.txt``` get installed in the endpoint container when it is initialized. More on that and examples [here](https://github.com/aws/sagemaker-tensorflow-serving-container#prepost-processing).\n",
    "\n",
    "The export directory structure should look like this:\n",
    "\n",
    "```\n",
    "notebooks\n",
    "     ...\n",
    "     ├─ deploy-megadetector.ipynb\n",
    "     ├─ export\n",
    "           └─ Servo\n",
    "                 └─ 1\n",
    "                       └─ saved_model.pb\n",
    "                       └─ variables\n",
    "           └─ code\n",
    "                 └─ inference.py\n",
    "                 └─ requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir export/Servo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir export/Servo/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir export/Servo/1/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../models/megadetector/code export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tar the entire directory and upload to S3\n",
    "Yeeehaw now we're read to zip it all up and upload it to s3..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Deploy the trained model\n",
    "\n",
    "There are [two ways to deploy models to sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html), using the AWS Sagemaker Python SDK (what we use below), or using the AWS SDK for Python (Boto 3). Boto 3 offers more lower level configuration controls. Documentation on using the Sagemaker Python SDK for deployment can be found [here](https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploy-to-a-sagemaker-endpoint). The ```model.deploy()``` function returns a predictor that you can use to test inference on right away.\n",
    "\n",
    "TODO: \n",
    "- look into using Elastic Inference (https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html) for low-cost fast inference without using a GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'msft-megadetector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                        name = endpoint_name,\n",
    "                        role = role,\n",
    "                        framework_version = '1.14',\n",
    "                        entry_point = 'inference.py',\n",
    "                        source_dir='export/code'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count = 1,\n",
    "                                   instance_type = 'ml.c5.xlarge',\n",
    "                                   accelerator_type='ml.eia2.medium',\n",
    "                                   endpoint_name = endpoint_name\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a predictor from the endpoint\n",
    "This is only necessary if you didn't just deploy an endpoint and create a predictor in the step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "predictor = sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint using a boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "TEST_IMG = '../input/sample-img.jpg'\n",
    "# RENDER_THRESHOLD = 0.8\n",
    "# MODEL_NAME = \"megadetector\"\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open(TEST_IMG, 'rb') as fd:\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image', \n",
    "        Body=fd\n",
    "    )\n",
    "\n",
    "    response_body = response['Body']\n",
    "    print(response_body.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}