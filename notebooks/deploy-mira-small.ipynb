{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy MIRA Small\n",
    "\n",
    "A lot of the steps below are taken from [this blog post](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/) which explains how to take advantage of Amazon SageMaker deployment capabilities, such as selecting the type and number of instances, performing A/B testing, and Auto Scaling. Auto Scaling clusters are spread across multiple Availability Zones to deliver high performance and high availability.\n",
    "\n",
    "In this notebook we'll be deploying the Small MIRA model, or \"Ratter\", which was trained to detect smaller Santa Cruz Island animals (classes are \"Rodent\", \"Empty\"). The Keras model graph and weight files can be downloaded [here](https://github.com/tnc-ca-geo/mira/tree/master/mira_models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up\n",
    "\n",
    "If you're already reading this in a Sagemaker Notebook instance, just execute the code block below to get the Sagemaker execution role.\n",
    "\n",
    "If not, and you need to set up the Sagemaker Notebook, in the AWS Management Console go to the Amazon SageMaker console. We are using Elastic Inference, which is not available in all regions, so make sure you're in a region that supports it (e.g. us-west-2). From the console, choose Notebook Instances, and create a new notebook instance. Associate it with the animl-ml git repo (https://github.com/tnc-ca-geo/animl-ml), and set the kernel to conda_tensorflow_p36.\n",
    "\n",
    "The ```get_execution_role``` function retrieves the AWS Identity and Access Management (IAM) role you created at the time of creating your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the Keras model using the JSON and weights file\n",
    "If you've alredy converted the Keras model and prepared the ```export/``` directory, you can skip this and jump to step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import model_from_json\n",
    "\n",
    "!mkdir keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the json file ([model-ratter-20181130.json](https://github.com/tnc-ca-geo/mira/blob/master/mira_models/model-ratter-20181130.json)) and model weights ([model-ratter-20181130.hdf5](https://github.com/tnc-ca-geo/mira/blob/master/mira_models/model-ratter-20181130.hdf5)) from the MIRA repo mentioned above. Navigate to ```keras_model``` from the Jupyter notebook home, and upload both model.json and model-weights.h5 files (using the “Upload” menu on the Jupyter notebook home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls keras_model\n",
    "\n",
    "json_file = open('/home/ec2-user/SageMaker/animl-ml/notebooks/keras_model/'+'model-ratter-20181130.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights('/home/ec2-user/SageMaker/animl-ml/notebooks/keras_model/model-ratter-20181130.hdf5')\n",
    "\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Export the Keras model to the TensorFlow ProtoBuf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# Note: This directory structure will need to be followed - see notes for the next section\n",
    "model_version = '1'\n",
    "export_dir = 'export/Servo/' + model_version\n",
    "\n",
    "# Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "builder = builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "print('loaded_model.input: {}'.format(loaded_model.input))\n",
    "print('loaded_model.output: {}'.format(loaded_model.output))\n",
    "signature = predict_signature_def(\n",
    "    inputs={\"inputs\": loaded_model.input}, outputs={\"score\": loaded_model.output})\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    # Save the meta graph and variables\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Copy pre & post processing scrips into export directory\n",
    "\n",
    "Copy inference.py and requirements.txt into ```export/code/```. \n",
    "\n",
    "```inference.py``` is a pre/post processing script, and its dependencies in ```requirements.txt``` get installed in the endpoint container when it is initialized. More on that and examples [here](https://github.com/aws/sagemaker-tensorflow-serving-container#prepost-processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../models/mira-small/code export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your export directory structure should now look like this:\n",
    "\n",
    "```\n",
    "notebooks\n",
    "     ...\n",
    "     ├─ deploy-mira-small.ipynb\n",
    "     ├─ export\n",
    "           └─ Servo\n",
    "                 └─ 1\n",
    "                       └─ saved_model.pb\n",
    "                       └─ variables\n",
    "                           └─ variables.data-00000-of-00001\n",
    "                           └─ variables.index\n",
    "           └─ code\n",
    "                 └─ inference.py\n",
    "                 └─ requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Tar the entire directory and upload to S3\n",
    "Yeeehaw now we're read to zip it all up and upload it to s3..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')"
   ]
  },
  {
   "source": [
    "Do A quick sanity check to make sure the boto region is on in which Elastic Inference is supported (e.g. us-west-2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker_session.boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Deploy the trained model\n",
    "\n",
    "There are [two ways to deploy models to sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html), using the AWS Sagemaker Python SDK (what we use below), or using the AWS SDK for Python (Boto 3). Boto 3 offers more lower level configuration controls. Documentation on using the Sagemaker Python SDK for deployment can be found [here](https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploy-to-a-sagemaker-endpoint). The ```model.deploy()``` function returns a predictor that you can use to test inference on right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'mira-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                        name = endpoint_name,\n",
    "                        role = role,\n",
    "                        framework_version = '1.14',\n",
    "                        entry_point = 'inference.py',\n",
    "                        source_dir='export/code'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count = 1,\n",
    "                                   instance_type = 'ml.c5.xlarge',\n",
    "                                   accelerator_type='ml.eia2.medium',\n",
    "                                   endpoint_name = endpoint_name\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "from io import BytesIO\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "CLASSES = ['rodent', 'empty']\n",
    "TEST_INPUT = [\n",
    "    {\n",
    "        'file': 'SageMaker/animl-ml/input/sample-img.jpg',\n",
    "        'detections': [{\n",
    "            'category': '1',\n",
    "            'conf': 0.970706,\n",
    "            'bbox': [0.536007, 0.434649, 0.635773, 0.543599]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        'file': 'SageMaker/animl-ml/input/sample-img-fox.jpg',\n",
    "        'detections': [{\n",
    "            'category': '1',\n",
    "            'conf': 0.970706,\n",
    "            'bbox': [0.24598, 0.475871, 0.567701, 0.636399]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        'file': 'SageMaker/animl-ml/input/sample-img-fox-2.jpg',\n",
    "        'detections': [{\n",
    "            'category': '1',\n",
    "            'conf': 0.970706,\n",
    "            'bbox': [0.229707, 0.256563, 0.646652, 0.542365]\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        'file': 'SageMaker/animl-ml/input/sample-img-rodent.jpg',\n",
    "        'detections': [{\n",
    "            'category': '1',\n",
    "            'conf': 0.99954,\n",
    "            'bbox': [0.0580928, 0.383797, 0.409989, 0.663712]\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# prep image\n",
    "image = Image.open(os.path.join(HOME, TEST_INPUT[3]['file']))\n",
    "# imshow(np.asarray(image))\n",
    "\n",
    "# Megadetector bbox is [ymin, xmin, ymax, xmax] in relative values\n",
    "# convert to tuple (xmin, ymin, xmax, ymax) in pixel values \n",
    "W, H = image.size\n",
    "box = TEST_INPUT[3]['detections'][0]['bbox']\n",
    "boxpx = (int(box[1]*W), int(box[0]*H), int(box[3]*W), int(box[2]*H))\n",
    "crp = image.crop(boxpx)\n",
    "imshow(np.asarray(crp))\n",
    "\n",
    "# convert to bytes\n",
    "buf = BytesIO()\n",
    "crp.save(buf, format='JPEG')\n",
    "\n",
    "# invoke endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = 'application/x-image', \n",
    "    Body = buf.getvalue()\n",
    ")\n",
    "\n",
    "# parse response\n",
    "response_body = response['Body'].read()\n",
    "response_body = response_body.decode('utf-8')\n",
    "pred = json.loads(response_body)['predictions'][0]\n",
    "\n",
    "output = {}\n",
    "for i in range(len(pred)): output[CLASSES[i]] = float(pred[i])\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}