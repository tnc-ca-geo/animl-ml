{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows an example of loading two pytorch models to do the same inference. Pytorch has two main ways of exporting models. \n",
    "\n",
    "1. Exporting the weights, which requires the model definition in code to load the weights\n",
    "2. Exporting the model with the structure and weights combined in a compiled file format: Torchscript (Torchscript itself has two flavors we won't get into)\n",
    "\n",
    "### exporting the model for inference\n",
    "see the readme for instructions on downloading the model weights.\n",
    "clone the yolov5 repo (from [this commit](https://github.com/ultralytics/yolov5/blob/6dd6aea0866ba9115d38e2989f59cf1039b3c9d2/export.py) if master doesn't work). then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'export.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python ../api/yolov5/export.py --weights ../models/megadetectorv5/md_v5a.0.0.pt --img 640 --batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# TNC test images (local):\n",
    "test_images_local = []\n",
    "test_images_dir = os.path.abspath(os.path.join(os.path.abspath(''), '..', 'input'))\n",
    "local_image_files = [\n",
    "    'sample-img-empty.jpg',\n",
    "    'sample-img.jpg',\n",
    "    'sample-img-skunk-large.jpg',\n",
    "    'sample-img-rodent.jpg',\n",
    "    'sample-img-fox.jpg',\n",
    "    'sample-img-fox-2.jpg',\n",
    "]\n",
    "for fil in local_image_files:\n",
    "    test_images_local.append(os.path.join(test_images_dir, fil))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchscript model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.jit.load('../models/megadetectorv5/md_v5a.0.0.torchscript')\n",
    "# set model parameters can go in inference basehandler subclass\n",
    "model.conf = 0.10  # NMS confidence threshold\n",
    "model.iou = 0.45  # NMS IoU threshold\n",
    "model.agnostic = False  # NMS class-agnostic\n",
    "model.multi_label = False  # NMS multiple labels per box\n",
    "model.max_det = 1000  # maximum number of detections per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall time inference for single 2048x2048 image on cpu is 5.55 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.6 s, sys: 3.48 s, total: 39.1 s\n",
      "Wall time: 7.94 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rave/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import skimage.io as skio\n",
    "impath=test_images_local[2]\n",
    "arr = skio.imread(impath)\n",
    "padded_arr = three_channel_arr_to_shape(arr, (2048,2048))\n",
    "im = torch.from_numpy(padded_arr)\n",
    "im = torch.moveaxis(im,2,0).to(\"cpu\").float()[None,...]\n",
    "result = model(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the shape of the output is not what we would expect, the lists of coordinates, predicted category ids, and confidence scores need to be derived from this result. this is handled in mdv5_handler.py in the api/megadetectorv5 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 261120, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with the yolov5 code to load the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = impath=test_images_local[-2]\n",
    "impath = \"../input/sample-img-skunk-large.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wall time inference for single 2048x2048 image on gpu is .5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yolov5.models.common.Detections at 0x7fb8a0b1ae50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yolov5\n",
    "yolomodel = yolov5.load('../models/megadetectorv5/md_v5a.0.0.pt')\n",
    "result_lst = yolomodel(impath)\n",
    "result_lst"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6842b0dd326cf6d5fadc2749907529d24dd9503c7e6290183b48a4093d0f793"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
