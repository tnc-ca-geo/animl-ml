{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"/Users/nathaniel.rindlaub/Downloads/nzdoc-test-images/stoat_3C47B454-8308-415F-89C5-3D0B94E87952.JPG\"\n",
    "BBOX = [0.5696394443511963,0.2648513615131378,0.8928725123405457,0.6756160855293274]\n",
    "\n",
    "IMG_SIZE = 480\n",
    "BUFFER = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nzdoc_crop_and_resize(img, bbox_rel, out_size=IMG_SIZE, image_buffer=BUFFER):\n",
    "    \"\"\"\n",
    "    Crops an image to the bounding box, and resizes to a square.\n",
    "\n",
    "    NOTE: this replicates the behavior of NZ DOC's Crop_Resize_Image.py script, \n",
    "    in which they they crop to the dimensions of the bounding box and resize the\n",
    "    arbitrary image shape to a square, thus distorting aspect ratio.\n",
    "\n",
    "    It might be worth consulting with their ML engineers to ask whether they \n",
    "    think MSFT's approach (cropping to smallest square that encloses the bbox,\n",
    "    and then resizing to preserve aspect ratio) might be better? Might also\n",
    "    require retraining the model\n",
    "\n",
    "    Args:\n",
    "        img: PIL.Image.Image object, already loaded\n",
    "        bbox_rel: list or tuple of float, [ymin, xmin, ymax, xmax] all in\n",
    "            relative coordinates\n",
    "\n",
    "    Returns: cropped image\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"cropping image. original image size: {img.size}\")\n",
    "    print(f\"bbox_rel: {bbox_rel}\")\n",
    "\n",
    "    img_w, img_h = img.size\n",
    "    # xmin = int(bbox_rel[1] * img_w)\n",
    "    # ymin = int(bbox_rel[0] * img_h)\n",
    "    # box_w = int((bbox_rel[3] - bbox_rel[1]) * img_w)\n",
    "    # box_h = int((bbox_rel[2] - bbox_rel[0]) * img_h)\n",
    "\n",
    "    left = bbox_rel[1]\n",
    "    top = bbox_rel[0]\n",
    "    right = bbox_rel[3]\n",
    "    bottom = bbox_rel[2]\n",
    "\n",
    "    print(f\"left: {left}, top: {top}, right: {right}, bottom: {bottom}\")\n",
    "\n",
    "\n",
    "    # add buffer\n",
    "    left = left - image_buffer\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    top = top - image_buffer\n",
    "    if top < 0:\n",
    "        top = 0\n",
    "    right = right + image_buffer\n",
    "    if right > 1:\n",
    "        right = 1\n",
    "    bottom = bottom + image_buffer\n",
    "    if bottom > 1:\n",
    "        bottom = 1\n",
    "\n",
    "    # Image.crop() takes box=[left, upper, right, lower]\n",
    "    img = img.crop((\n",
    "        int(left * img_w),\n",
    "        int(top * img_h),\n",
    "        int(right * img_w),\n",
    "        int(bottom * img_h)\n",
    "    ))\n",
    "    img = img.resize((out_size, out_size), resample=3)\n",
    "    print(f\"cropped & resized image size: {img.size}\")\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mira_crop(img, bbox_rel):\n",
    "    \"\"\"\n",
    "    Crops an image to the tightest square enclosing each bounding box. \n",
    "    This will always generate a square crop whose size is the larger of the \n",
    "    bounding box width or height. In the case that the square crop boundaries \n",
    "    exceed the original image size, the crop is padded with 0s.\n",
    "\n",
    "    Args:\n",
    "        img: PIL.Image.Image object, already loaded\n",
    "        bbox_rel: list or tuple of float, [ymin, xmin, ymax, xmax] all in\n",
    "            relative coordinates\n",
    "\n",
    "    Returns: cropped image\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"cropping image. original image size: {img.size}\")\n",
    "\n",
    "    img_w, img_h = img.size\n",
    "    xmin = int(bbox_rel[1] * img_w)\n",
    "    ymin = int(bbox_rel[0] * img_h)\n",
    "    box_w = int((bbox_rel[3] - bbox_rel[1]) * img_w)\n",
    "    box_h = int((bbox_rel[2] - bbox_rel[0]) * img_h)\n",
    "\n",
    "    # expand box width or height to be square, but limit to img size\n",
    "    box_size = max(box_w, box_h)\n",
    "    xmin = max(0, min(\n",
    "        xmin - int((box_size - box_w) / 2),\n",
    "        img_w - box_w))\n",
    "    ymin = max(0, min(\n",
    "        ymin - int((box_size - box_h) / 2),\n",
    "        img_h - box_h))\n",
    "    box_w = min(img_w, box_size)\n",
    "    box_h = min(img_h, box_size)\n",
    "\n",
    "    # if box_w == 0 or box_h == 0:\n",
    "    #     tqdm.write(f'Skipping size-0 crop (w={box_w}, h={box_h}) at {save}')\n",
    "    #     return False\n",
    "\n",
    "    # Image.crop() takes box=[left, upper, right, lower]\n",
    "    crop = img.crop(box=[xmin, ymin, xmin + box_w, ymin + box_h])\n",
    "\n",
    "    if (box_w != box_h):\n",
    "        # pad to square using 0s\n",
    "        crop = ImageOps.pad(crop, size=(box_size, box_size), color=0)\n",
    "\n",
    "    print(f\"cropped image size: {crop.size}\")\n",
    "\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "with Image.open(IMG_PATH) as im:\n",
    "    cropped = nzdoc_crop_and_resize(im, BBOX)\n",
    "    # cropped = mira_crop(im, BBOX)\n",
    "    display(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cropped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08c26255c77ed5e85ffb773e36a7d95a8cddae2707c753f0e717afc8204cad2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
